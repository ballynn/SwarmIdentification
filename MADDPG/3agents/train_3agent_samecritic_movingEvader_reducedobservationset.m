%Train 3 pursuers to take a path toward a single stationary evader
%by: Lynn Sargent, University of Miami, 2022
clear
clc
  %% Create Environment
  %Number of Observations
  rng(0)
  env  = Environment_MovingEvader;
  numObs = 9;
  
  %Number of Actions
  numAct = 3;
  
  % I/O specifications for each agent
  oinfo = rlNumericSpec([numObs,1],'LowerLimit',0,'UpperLimit',360);
  ainfo = rlNumericSpec([1,1],'LowerLimit',-3,'UpperLimit',3);
%  oinfo_critic = rlNumericSpec([numObs*3,1],'LowerLimit',0,'UpperLimit',360);
  oinfo_critic = rlNumericSpec([15,1],'LowerLimit',0,'UpperLimit',360);
  ainfo_critic = rlNumericSpec([3,1],'LowerLimit',-5,'UpperLimit',5);
  
  oinfo.Name = 'observations';
  ainfo.Name = 'angle1';

 %% Critic Network

statePath = [
    %featureInputLayer(numObs*3, 'Normalization', 'none', 'Name', 'observation')
    featureInputLayer(15, 'Normalization', 'none', 'Name', 'observation')
    fullyConnectedLayer(128, 'Name', 'CriticStateFC1')
    reluLayer('Name', 'CriticRelu1')
    fullyConnectedLayer(64, 'Name', 'CriticStateFC2')];
actionPath = [
    featureInputLayer(1, 'Normalization', 'none', 'Name', 'action')
    fullyConnectedLayer(64, 'Name', 'CriticActionFC1', 'BiasLearnRateFactor', 0)];
commonPath = [
    additionLayer(2,'Name', 'add')
    reluLayer('Name','CriticCommonRelu')
    fullyConnectedLayer(1, 'Name', 'CriticOutput')];

criticNetwork = layerGraph(statePath);
criticNetwork = addLayers(criticNetwork, actionPath);
criticNetwork = addLayers(criticNetwork, commonPath);
criticNetwork = connectLayers(criticNetwork,'CriticStateFC2','add/in1');
criticNetwork = connectLayers(criticNetwork,'CriticActionFC1','add/in2');

criticOptions = rlRepresentationOptions('LearnRate',5e-03);

critic1 = rlQValueRepresentation(criticNetwork,oinfo_critic,ainfo,'Observation',{'observation'},'Action',{'action'},criticOptions);
critic2 = rlQValueRepresentation(criticNetwork,oinfo_critic,ainfo,'Observation',{'observation'},'Action',{'action'},criticOptions);
critic3 = rlQValueRepresentation(criticNetwork,oinfo_critic,ainfo,'Observation',{'observation'},'Action',{'action'},criticOptions);

%% Actor Networks
actorNetwork1 = [
    featureInputLayer(numObs, 'Normalization', 'none', 'Name', 'observation')
    fullyConnectedLayer(256, 'Name', 'ActorFC1')
    reluLayer('Name', 'ActorRelu1')
    fullyConnectedLayer(128, 'Name', 'ActorFC2')
    reluLayer('Name', 'ActorRelu2')
    fullyConnectedLayer(1, 'Name', 'ActorFC3')
    tanhLayer('Name', 'ActorTanh1')];

actorOptions1 = rlRepresentationOptions('LearnRate',5e-03);

actor1 = rlDeterministicActorRepresentation(actorNetwork1,oinfo,ainfo,'Observation',{'observation'},'Action',{'ActorTanh1'},actorOptions1);
actor2 = rlDeterministicActorRepresentation(actorNetwork1,oinfo,ainfo,'Observation',{'observation'},'Action',{'ActorTanh1'},actorOptions1);
actor3 = rlDeterministicActorRepresentation(actorNetwork1,oinfo,ainfo,'Observation',{'observation'},'Action',{'ActorTanh1'},actorOptions1);

actor1Target = actor1;
actor2Target = actor2;
actor3Target = actor3;

%% Experience Buffer
%Create Experience buffer for storing agent experiences
myBuffer.bufferSize = 1e6;
myBuffer.bufferIndex = 0;
myBuffer.currentBufferLength = 0;
myBuffer.observation = zeros(numObs*3,myBuffer.bufferSize);
myBuffer.nextObservation = zeros(numObs*3,myBuffer.bufferSize);
myBuffer.action = zeros(numAct,myBuffer.bufferSize);
myBuffer.reward = zeros(numAct,myBuffer.bufferSize);

%Create Model experience buffer for storing experiences generated by the
%models

myModelBuffer.buffersize = 1e2;
myModelBuffer.bufferIndex = 0;
myModelBuffer.currentBufferLength = 0;
myModelBuffer.observation = zeros(numObs*3,myBuffer.bufferSize);
myModelBuffer.nextObservation = zeros(numObs*3,myBuffer.bufferSize);
myModelBuffer.action = zeros(numAct,myBuffer.bufferSize);
myModelBuffer.reward = zeros(numAct,myBuffer.bufferSize);

%% Training options
maxsteps = 2500;
numEpisodes = 50000;
maxStepsPerEpisode = 400;
maxTrainingEpisodes = 250;
discountFactor = 0.99;
aveWindowSize = 100;
trainingTerminationValue = 5000;
episodeCumulativeRewardVector1 = [];
episodeCumulativeRewardVector2 = [];
episodeCumulativeRewardVector3 = [];
epsilon = 1;
epsilonMin = 0.01;
epsilonDecay = 0.005;
totalStepCt = 0;
warmStartSamples = 250;

[trainingPlot1,lineReward1,lineAveReward1,QValue1] = hBuildFigure(1);
[trainingPlot2,lineReward2,lineAveReward2,QValue2] = hBuildFigure(2);
[trainingPlot3,lineReward3,lineAveReward3,QValue3] = hBuildFigure(3);

modelTrainedAtleastOnce = 0;
buildminibatch = 0;
realRatio = 1;
CounterSoftUpdate = 0;
SoftUpdate = 20;

%% Custom Training Loop

% Enable the training visualization plot.
set(trainingPlot1,'Visible','on');
set(trainingPlot2,'Visible','on');
set(trainingPlot3,'Visible','on');


% Train the policy for the maximum number of episodes or until the average
% reward indicates that the policy is sufficiently trained.
for episodeCt = 1:numEpisodes
    %Initialize Environment and zero out veriables
    obs = reset(env);
    episodeReward = zeros(maxStepsPerEpisode,3);
    episodeQVal = zeros(maxStepsPerEpisode,3);
    prevobsbuffer = zeros(numObs,3,maxStepsPerEpisode);

    %Update Counter for Target Network Updates
    if CounterSoftUpdate < SoftUpdate
        CounterSoftUpdate = CounterSoftUpdate + 1;
    else
        CounterSoftUpdate = 1;
    end

    %Loop Over the Episode
    for stepCt = 1:maxStepsPerEpisode

        if myModelBuffer.buffersize <= totalStepCt
            totalStepCt = 0;
        end

        totalStepCt = totalStepCt + 1;

        % Compute an action using the policy based on the current 
        % observation.
   
        action1 = getAction(actor1Target,{obs([1:9])});
        action2 = getAction(actor2Target,{obs([10:18])});
        action3 = getAction(actor3Target,{obs([19:27])});

        %Compute the Q Value for the step for plotting only
        criticobs = reshape(obs,[numObs*3,1]);
        criticobs([10,11,14,15,16,18,19,20,23,24,25,27],:,:) = [];

        QVal1 = getValue(critic1,{criticobs},action1);
        QVal2 = getValue(critic2,{criticobs},action2);
        QVal3 = getValue(critic3,{criticobs},action3);
        
        % Apply the action to the environment and obtain the resulting
        % observation and reward.
        if stepCt == 1
           prevobsbuffer(:,:,stepCt) = prevobsbuffer(:,:,stepCt) ;
        else
            prevobsbuffer(:,:,stepCt) = [Observation1',Observation2', Observation3'];
        end
        
        prevobs = reshape(prevobsbuffer(:,:,stepCt),[numObs*3,1]);

        %Compute next step
        [Observation1, Observation2, Observation3, Reward1, Reward2, Reward3, IsDone , ~] = step(env, action1{1}, action2{1}, action3{1});
        
        % Store the action, observation, and reward experiences in buffers.
        obs = reshape([Observation1',Observation2', Observation3'],[numObs*3,1]);
        myBuffer = storeExperience(myBuffer,prevobs,[action1{1}, action2{1}, action3{1}],obs,[Reward1,Reward2,Reward3],IsDone);
        episodeReward(stepCt,:) = [Reward1,Reward2,Reward3];
        episodeQVal(stepCt,:) = [QVal1, QVal2, QVal3];

        %Plot the Episode
%          plot([Observation1(3),Observation2(3),Observation3(3)],[Observation1(4),Observation2(4),Observation3(4)],'bo')
%          hold on
%          plot(Observation1(1),Observation1(2),'ro')
        

        % Stop if a terminal condition is reached.
        if IsDone == 1
            break;
        end
    end

%% Update Critic and Actor Networks from MiniBatch
%Set Batch size to max steps per episode
 batchSize = maxStepsPerEpisode;
%If the Buffer doesn't have as many episodes as the batch size yet set it
%to the length of the buffer.
 if myBuffer.currentBufferLength < batchSize
    batchSize = myBuffer.currentBufferLength;
 end

    for batchnum = 1:batchSize
       % Sample MiniBatch from Experience Buffer
       [sampledObservation,sampledAction,sampledNextObservation,sampledReward,sampledIsDone] = ...
           sampleMinibatch(modelTrainedAtleastOnce,realRatio,batchSize,myBuffer);

       %Compute Reduced Sampled observation set for critic
        reducedsampledObservation = sampledObservation;
        reducedsampledObservation([10,11,14,15,16,18,19,20,23,24,25,27],:,:) = [];

        reducedsampledNextObservation = sampledNextObservation;
        reducedsampledNextObservation([10,11,14,15,16,18,19,20,23,24,25,27],:,:) = [];

        %Compute next actions
        sampledNextAction(1,batchnum) = getAction(actor1Target,{sampledNextObservation(([1:9]),batchnum)});
        sampledNextAction(2,batchnum)  = getAction(actor2Target,{sampledNextObservation(([10:18]),batchnum)});
        sampledNextAction(3,batchnum) = getAction(actor3Target,{sampledNextObservation(([19:27]),batchnum)});

        act1(batchnum) = cell2mat(sampledNextAction(1,batchnum));
        act2(batchnum) = cell2mat(sampledNextAction(2,batchnum));
        act3(batchnum) = cell2mat(sampledNextAction(3,batchnum));

       % Compute Q Value for new state
       val1(batchnum) = getValue(critic1, reducedsampledNextObservation(:,batchnum), sampledNextAction(1,batchnum));
       val2(batchnum) = getValue(critic2, reducedsampledNextObservation(:,batchnum), sampledNextAction(2,batchnum));
       val3(batchnum) = getValue(critic3, reducedsampledNextObservation(:,batchnum), sampledNextAction(3,batchnum));

       % Compute target Q value
       y1(:,batchnum) = sampledReward(1,batchnum) + discountFactor*(val1(batchnum));
       y2(:,batchnum)= sampledReward(2,batchnum) + discountFactor*(val2(batchnum));
       y3(:,batchnum)= sampledReward(3,batchnum) + discountFactor*(val3(batchnum));
    end

    %% Update Critic
    %Compute loss function
    lossData.batchSize = batchSize;
    lossData.actInfo = ainfo_critic;
    %     lossData.actionBatch = actionBuffer;
    lossData.actionBatch = sampledAction(1,:);
    lossData.targetQValues = y1;

    %Compute the gradient
    CriticGradient = gradient(critic1,@criticLossFunction, ...
                    [{reshape(reducedsampledObservation,[15,1,batchSize])},{reshape(sampledAction(1,:),[1,1,batchSize])}],lossData);
   
    %Update the Critic by the gradient
    critic1 = optimize(critic1,CriticGradient);
    
    %Complete for other two Critics
    lossData.actionBatch = sampledAction(2,:);
    lossData.targetQValues = y2;

    CriticGradient = gradient(critic2,@criticLossFunction, ...
                    [{reshape(reducedsampledObservation,[15,1,batchSize])},{reshape(sampledAction(2,:),[1,1,batchSize])}],lossData);
    critic2 = optimize(critic2,CriticGradient);

    lossData.actionBatch = sampledAction(3,:);
    lossData.targetQValues = y3;

     CriticGradient = gradient(critic3,@criticLossFunction, ...
                    [{reshape(reducedsampledObservation,[15,1,batchSize])},{reshape(sampledAction(2,:),[1,1,batchSize])}],lossData);
    critic3 = optimize(critic3,CriticGradient);

    %% Update Actor
%     %Compute DQ/da
%     DQda1 = gradient(val1, act1);
%     DQda2 = gradient(val2, act2);
%     DQda3 = gradient(val3, act3);
% 
%     %Compute da/dtheta
%     daDtheta1 = gradient(actor1, 'output-input', act1);

    %Compute Actor loss
    Critic2InputGradient1 = gradient(critic1, 'output-input', [{reshape(reducedsampledObservation,[15,1,batchSize])},{reshape((sampledAction(1,:)),[1,1,batchSize])}]);
    Critic2InputGradient2 = gradient(critic2, 'output-input', [{reshape(reducedsampledObservation,[15,1,batchSize])},{reshape((sampledAction(2,:)),[1,1,batchSize])}]);
    Critic2InputGradient3 = gradient(critic3, 'output-input', [{reshape(reducedsampledObservation,[15,1,batchSize])},{reshape((sampledAction(3,:)),[1,1,batchSize])}]);

    actor1loss = -mean(Critic2InputGradient1{2}(1,:));
    actor2loss = -mean(Critic2InputGradient2{2}(1,:));
    actor3loss = -mean(Critic2InputGradient3{2}(1,:));
    
    %Compute Actor1 loss
    LossData.aloss = actor1loss;
    
    actor1Gradient = gradient(actor1,@actorLossFunction,...
            {reshape(sampledObservation([1:9],:),[numObs,1,batchSize])},LossData);
    
    %Compute Actor2 loss
    LossData.aloss = actor2loss;

    actor2Gradient = gradient(actor2,@actorLossFunction,...
           {reshape(sampledObservation([10:18],:),[numObs,1,batchSize])},LossData);
    
    %Compute Actor3 loss
    LossData.aloss = actor3loss;
    
    actor3Gradient = gradient(actor3,@actorLossFunction,...
            {reshape(sampledObservation([19:27],:),[numObs,1,batchSize])},LossData);
        
    %Update the actor network using the computed gradients.
    actor1 = optimize(actor1,actor1Gradient);
    actor2 = optimize(actor2,actor2Gradient);
    actor3 = optimize(actor3,actor3Gradient);
    
       
%    Soft Update the target network
    if CounterSoftUpdate == SoftUpdate
        actor1Target = syncParameters(actor1Target,actor1,0.01);
        actor2Target = syncParameters(actor2Target,actor2,0.01);
        actor3Target = syncParameters(actor3Target,actor3,0.01);
    end
    
    
      
   %% Update the training visualization.
    %Fig1
    episodeCumulativeReward1 = sum(episodeReward(:,1));
    episodeCumulativeRewardVector1 = cat(2,...
        episodeCumulativeRewardVector1,episodeCumulativeReward1);
    movingAveReward1 = movmean(episodeCumulativeRewardVector1,...
        aveWindowSize,2);
    episodeQVal1 = max(episodeQVal(:,1));
    addpoints(lineReward1,episodeCt,episodeCumulativeReward1);
    addpoints(lineAveReward1,episodeCt,movingAveReward1(end));
    addpoints(QValue1,episodeCt,episodeQVal1);
    drawnow;

    %Fig2
    episodeCumulativeReward2 = sum(episodeReward(:,2));
    episodeCumulativeRewardVector2 = cat(2,...
        episodeCumulativeRewardVector2,episodeCumulativeReward2);
    movingAveReward2 = movmean(episodeCumulativeRewardVector2,...
        aveWindowSize,2);
    episodeQVal2 = max(episodeQVal(:,2));
    addpoints(lineReward2,episodeCt,episodeCumulativeReward2);
    addpoints(lineAveReward2,episodeCt,movingAveReward2(end));
    addpoints(QValue2,episodeCt,episodeQVal2);
    drawnow;

    %Fig3
    episodeCumulativeReward3 = sum(episodeReward(:,3));
    episodeCumulativeRewardVector3 = cat(2,...
        episodeCumulativeRewardVector3,episodeCumulativeReward3);
    movingAveReward3 = movmean(episodeCumulativeRewardVector3,...
        aveWindowSize,2);
    episodeQVal3 = max(episodeQVal(:,3));
    addpoints(lineReward3,episodeCt,episodeCumulativeReward3);
    addpoints(lineAveReward3,episodeCt,movingAveReward3(end));
    addpoints(QValue3,episodeCt,episodeQVal3);
    drawnow;
    
    % 9. Terminate training if the network is sufficiently trained.
    if max(movingAveReward1) > trainingTerminationValue
        break
    end

end

%Run Model

%% Train Agent
%stats = train([agent1,agent2, agent3],env,trainingOptions);

%% Play the game with the trained agent
%simOptions = rlSimulationOptions('MaxSteps',maxsteps);
%experience = sim(env,[agent1 agent2 agent3],simOptions);
